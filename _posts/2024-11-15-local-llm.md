---
layout: post
title:  "How to run LLMs on a local (or remote) machine"
lang: en
tags: [en]
published: true
---



## End result

- Ollama instance with open-sourced LLM weights running on a local machine (or any machine on the network for that matter), accessible via open-webui running in a Docker container, or Python API on the same machine or remotely.

## Prequisites

- Root access to a VM with Ubuntu 24.04 LTS. Might work on other versions, but the instructions are for 24.04
- NVIDIA GPU with at least 5 GB of VRAM

If installed in WSL or on a Mac, adjustments will be needed.
For non-root environments, it is possible to unpack ollama binary in user's home directory and run systemd service with `systemd --user`.

## To do

- [ ] add instructions for non-root environments

## Steps to reproduce

- (optional) check the time and date on the machine, adjust the timezone if needed
```bash
date
sudo timedatectl set-timezone Asia/Shanghai
```
or `sudo dpkg-reconfigure tzdata`

- (optional) set up local mirrors
  ```bash
  sudo tee sudo nano /etc/apt/sources.list.d/ubuntu.sources <<- 'EOF'
  Types: deb
  URIs: https://mirrors.tuna.tsinghua.edu.cn/ubuntu
  Suites: noble noble-updates noble-backports
  Components: main restricted universe multiverse
  Signed-By: /usr/share/keyrings/ubuntu-archive-keyring.gpg

  Types: deb
  URIs: http://security.ubuntu.com/ubuntu/
  Suites: noble-security
  Components: main restricted universe multiverse
  Signed-By: /usr/share/keyrings/ubuntu-archive-keyring.gpg

  EOF
  ```

- `sudo apt update && sudo apt upgrade -y`

- `sudo apt install -y git curl nvidia-smi nvtop`

- install NVIDIA drivers
  - the easy way
    ```bash
    sudo apt install ubuntu-drivers-common
    sudo ubuntu-drivers devices
    sudo apt install nvidia-driver-550
    sudo shutdown -r now
    ```
  - the recommended way
    ```bash
    curl -fSsL https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/3bf863cc.pub | gpg --dearmor | sudo tee /usr/share/keyrings/nvidia-drivers.gpg > /dev/null 2>&1
    sudo apt update
    sudo apt install dirmngr ca-certificates software-properties-common apt-transport-https dkms
    echo 'deb [signed-by=/usr/share/keyrings/nvidia-drivers.gpg] https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/ /' | sudo tee /etc/apt/sources.list.d/nvidia-drivers.list
    sudo apt update
    sudo apt install nvidia-driver-560
    apt list --installed | grep nvidia
    sudo shutdown -r now
    ```
- check with `nvidia-smi --query-gpu=compute_cap --format=csv`

- (optional) set up proxy (needed for docker installation). assuming the proxy is running on `localhost:12334`. change the address and the port number if needed
  ```bash
  export https_proxy=http://localhost:12334 http_proxy=http://localhost:12334
  ```
- install Docker (will be needed to run open-webui)
  ```bash
  # Add Docker's official GPG key:
  sudo apt-get update
  sudo apt-get install ca-certificates curl
  sudo install -m 0755 -d /etc/apt/keyrings
  sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
  sudo chmod a+r /etc/apt/keyrings/docker.asc

  # Add the repository to Apt sources:
  echo \
    "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \
    $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | \
    sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
  sudo apt-get update
  sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
  ```

- (optional) set up proxy for dockerhub
  ```bash
  sudo tee /etc/docker/daemon.json <<-'EOF'
  {
    "proxies": {
      "http-proxy": "http://localhost:12334",
      "https-proxy": "http://localhost:12334",
      "no-proxy": "*.test.example.com,.example.org,127.0.0.0/8"
    }
  }
  EOF
  sudo systemctl daemon-reload
  sudo systemctl restart docker
  ```

- (optional, but recommended) install Portainer 
  ```bash
  sudo docker volume create portainer_data
  sudo docker run -d -p 8000:8000 -p 9443:9443 --name portainer --restart=always -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer-ce:latest
  sudo docker ps
  ```
  - later update like this:
    ```bash
    sudo docker stop portainer
    sudo docker rm portainer
    sudo docker pull portainer/portainer-ce:2.21.4
    sudo docker run -d -p 8000:8000 -p 9443:9443 --name=portainer --restart=always -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer-ce:2.21.4
    ```

- install Ollama
  - review the script before running it. Might as well copy and paste it into a LLM and ask if there is any malicious code in it.
    ```bash
    curl https://ollama.com/install.sh -o install.sh
    nano install.sh
    ```
  - install with `curl -fsSL https://ollama.com/install.sh | sh`
  - edit the systemd unit file to bind the service to all IPs (or one in particular) on the machine
    ```bash
    sudo nano /etc/systemd/system/ollama.service
    ```
    - add the following line in the `[Service]` block
      ```bash
      [Service]
      Environment="OLLAMA_HOST=0.0.0.0"
      ```
    - `sudo systemctl daemon-reload`
    - `sudo systemctl restart ollama`
  - check if it's running
    - ```bash
      sudo ss -ntlp | grep ollama
      curl http://localhost:11434
      ```
- pull models from the Ollama hub (https://ollama.com/library)
  ```bash
  ollama list
  ollama pull llama3.2
  ```
- install open-webui
  ```bash
  sudo docker run -d --network=host  -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama
  ```
  - `sudo docker container ls`

  - navigate to http://localhost:8080 and register the admin account
  
  the database is located in `/var/lib/docker/volumes/open-webui`

  - later update with:
    ```bash
    sudo docker run --rm --volume /var/run/docker.sock:/var/run/docker.sock containrrr/watchtower --run-once open-webui
    ```

- disable proxy settings with `unset https_proxy http_proxy` or reopen the terminal

- (optional) install the Python API. use venv or conda to manage the environment, **do not install into the system directly**
  ```bash
  pip install ollama
  ```
  - test
    ```python
    from ollama import Client
    client = Client(host='http://localhost:11434')
    response = client.chat(model='llama3.2', messages=[
      {
        'role': 'user',
        'content': 'Why is the sky blue?',
      },
    ])
    ```
  


## Use

- https://localhost:9443 - Portainer web interface
- https://localhost:8000 - open-webui

## References

- https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository
- https://docs.portainer.io/start/install-ce/server/docker/linux
- https://github.com/open-webui/open-webui
- https://github.com/ollama/ollama-python